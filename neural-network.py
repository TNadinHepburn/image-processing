# Backprop on the Seeds Dataset
from random import seed
from random import randrange
from random import random
from csv import reader
from math import exp
 
# Load a CSV file
def load_csv(filename):
	dataset = list()
	with open(filename, 'r') as file:
		csv_reader = reader(file)
		for row in csv_reader:
			if not row:
				continue
			dataset.append(row)
	return dataset
 
# Convert string column to float
def str_column_to_float(dataset, column):
	for row in dataset:
		row[column] = float(row[column].strip())
 
# Convert string column to integer
def str_column_to_int(dataset, column):
	class_values = [row[column] for row in dataset]
	unique = set(class_values)
	lookup = dict()
	for i, value in enumerate(unique):
		lookup[value] = i
	for row in dataset:
		row[column] = lookup[row[column]]
	return lookup
 
# Find the min and max values for each column
def dataset_minmax(dataset):
	minmax = list()
	stats = [[min(column), max(column)] for column in zip(*dataset)]
	return stats
 
# Rescale dataset columns to the range 0-1
def normalize_dataset(dataset, minmax):
	for row in dataset:
		for i in range(len(row)-1):
			row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])
 
# Split a dataset into k folds
def cross_validation_split(dataset, n_folds):
	dataset_split = list()
	dataset_copy = list(dataset)
	fold_size = int(len(dataset) / n_folds)
	for i in range(n_folds):
		fold = list()
		while len(fold) < fold_size:
			index = randrange(len(dataset_copy))
			fold.append(dataset_copy.pop(index))
		dataset_split.append(fold)
	return dataset_split
 
# Calculate accuracy percentage
def accuracy_metric(actual, predicted):
	correct = 0
	for i in range(len(actual)):
		if actual[i] == predicted[i]:
			correct += 1
	return correct / float(len(actual)) * 100.0
 
# Evaluate an algorithm using a cross validation split
def evaluate_algorithm(dataset, algorithm, n_folds, *args):
	folds = cross_validation_split(dataset, n_folds)
	scores = list()
	for fold in folds:
		train_set = list(folds)
		train_set.remove(fold)
		train_set = sum(train_set, [])
		test_set = list()
		for row in fold:
			row_copy = list(row)
			test_set.append(row_copy)
			row_copy[-1] = None
		predicted = algorithm(train_set, test_set, *args)
		actual = [row[-1] for row in fold]
		accuracy = accuracy_metric(actual, predicted)
		scores.append(accuracy)
	return scores
 
# Calculate neuron activation for an input
def activate(weights, inputs):
	activation = weights[-1]
	for i in range(len(weights)-1):
		activation += weights[i] * inputs[i]
	return activation
 
# Transfer neuron activation
def transfer(activation):
	return 1.0 / (1.0 + exp(-activation))
 
# Forward propagate input to a network output
def forward_propagate(network, row):
	inputs = row
	for layer in network:
		new_inputs = []
		for neuron in layer:
			activation = activate(neuron['weights'], inputs)
			neuron['output'] = transfer(activation)
			new_inputs.append(neuron['output'])
		inputs = new_inputs
	return inputs
 
# Calculate the derivative of an neuron output
def transfer_derivative(output):
	return output * (1.0 - output)
 
# Backpropagate error and store in neurons
def backward_propagate_error(network, expected):
	for i in reversed(range(len(network))):
		layer = network[i]
		errors = list()
		if i != len(network)-1:
			for j in range(len(layer)):
				error = 0.0
				for neuron in network[i + 1]:
					error += (neuron['weights'][j] * neuron['delta'])
				errors.append(error)
		else:
			for j in range(len(layer)):
				neuron = layer[j]
				errors.append(neuron['output'] - expected[j])
		for j in range(len(layer)):
			neuron = layer[j]
			neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])
 
# Update network weights with error
def update_weights(network, row, l_rate):
	for i in range(len(network)):
		inputs = row[:-1]
		if i != 0:
			inputs = [neuron['output'] for neuron in network[i - 1]]
		for neuron in network[i]:
			for j in range(len(inputs)):
				neuron['weights'][j] -= l_rate * neuron['delta'] * inputs[j]
			neuron['weights'][-1] -= l_rate * neuron['delta']
 
# Train a network for a fixed number of epochs
def train_network(network, train, l_rate, n_epoch, n_outputs):
	for epoch in range(n_epoch):
		for row in train:
			outputs = forward_propagate(network, row)
			expected = [0 for i in range(n_outputs)]
			expected[row[-1]] = 1
			backward_propagate_error(network, expected)
			update_weights(network, row, l_rate)		
		print('>epoch=%d, lrate=%.3f' % (epoch, l_rate))
 
# Initialize a network
def initialize_network(n_inputs, n_hidden, n_outputs):
	network = list()
	hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]
	network.append(hidden_layer)
	output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]
	network.append(output_layer)
	return network
 
# Make a prediction with a network
def predict(network, row):
	outputs = forward_propagate(network, row)
	return outputs.index(max(outputs))
 
# Backpropagation Algorithm With Stochastic Gradient Descent
def back_propagation(train, test, l_rate, n_epoch, n_hidden):
	n_inputs = len(train[0]) - 1
	n_outputs = len(set([row[-1] for row in train]))
	network = initialize_network(n_inputs, n_hidden, n_outputs)
	train_network(network, train, l_rate, n_epoch, n_outputs)
	predictions = list()
	for row in test:
		prediction = predict(network, row)
		predictions.append(prediction)
	return(predictions)

seed(1)
testDataSet = [
[0.4599632618864581, 0.9894060687948345, 0.5281065088757396, 0.2145755963914167, 0.981555217937327, 0.6481820381598622, 0.4960398064415477, 0.635951447612015, 0.6671385942624045, 0.0, 0.8125706116597065, 0.6983628162920866, 0.1775979508994259, 0.21488494923217077, 0.7119098162655052, 0.0, 0.7755402566310774, 0.6791607951358509, 0.25891635458379264, 0.1845828099195165, 0.6807887162807826, 0.0, 0.7036410218387502, 0.662710567657813, 0.3165067767282756, 0.10046944233765455, 0.6793600217658285, 0.20705229557970375, 0.6616669381579162, 0.4160467944055041, 0.3358338516203081, 0.3525643065169463, 0.62304631833721, 12], 
[0.34187140352504436, 0.7512291425871319, 0.19644970414201185, 0.14020099983114795, 0.5402041088309499, 0.8050575359925526, 0.2362356546684794, 0.20127573884018182, 0.8401329812465183, 0.34373718825266025, 0.6438580124012917, 0.8064929804921164, 0.0, 0.4453771491210348, 0.7456307921359139, 0.30296969339196966, 0.6075480076797322, 0.8038672726017506, 0.1328967639216608, 0.26239600984979244, 0.8337403794028266, 0.25507881397207405, 0.5678095409951198, 0.8114876899376798, 0.4828049286014464, 0.05514130393007857, 0.7676074693313969, 0.32893413401680605, 0.5733175164108386, 0.48322151306758804, 0.7245732029821308, 0.397511444972781, 0.6604992881906411, 0], 
[0.3655329740073498, 0.6488334138453373, 0.30887573964497045, 0.32036719549610637, 0.3557158916239486, 0.7985142999877398, 0.7703117255053842, 0.28297319624038664, 0.9227487356470522, 1.0, 0.6938099790415941, 0.7103980399332664, 0.9527915568309447, 0.5237994852331412, 0.7318601158722957, 0.778834381600271, 0.9191926555321802, 0.659129352221283, 0.41392424416005114, 1.0, 0.6349378308986162, 0.5062454867794138, 1.0, 0.5895701142024887, 0.311468998680312, 0.9029995977136135, 0.804979290983922, 0.39555333885329774, 1.0, 0.3606796419701422, 0.28549400087108984, 0.9395762740240123, 0.8096813128107015, 11], 
[0.8421642752205128, 0.6980320477503047, 0.02366863905325446, 0.747182448735271, 0.5224247135075056, 0.7612886773873594, 0.839024789679552, 0.10736264497968763, 0.7892239526170781, 0.6380009518867636, 0.42809561402262974, 0.7875585715384373, 0.42892045311967686, 0.5991672766035564, 0.7302425824093016, 0.7039960077005494, 0.38066184620087806, 0.7959236700066147, 0.7310802063187306, 0.7521123323668907, 0.6440970774144951, 0.7287016013275238, 0.28042447211810567, 0.8453121973585701, 0.7328001174745267, 0.6012658904763825, 0.8199973303788504, 0.8074835568936382, 0.20681067401782274, 0.5572663987670456, 0.7422793226197492, 0.0, 0.8724220647779257, 3], 
[0.19613953304008103, 0.6110229407016073, 0.09792899408284024, 0.04899167132720773, 0.28569397624090237, 0.7622670055003491, 0.680713461090594, 0.29588077152585807, 0.8767318303180767, 0.1358898239096988, 0.34334783723892326, 0.7937122287283007, 0.2196121353862125, 0.5971910760723843, 0.8029230135922217, 0.11979497279539617, 0.49217511610979975, 0.831077602814669, 0.24725771913005398, 0.782412592361343, 0.8922652416342199, 0.0778118127514143, 0.5247268329907689, 0.963347521769712, 0.19541289169642206, 0.6641722290241299, 1.0, 0.2033365448700246, 0.5282965688714931, 0.6971185446447346, 0.74924298908909, 0.3880244105129347, 1.0, 1], 
[0.5311922279020278, 1.0, 0.8076923076923078, 0.4321109280954655, 0.9815389640052917, 0.7395493588248189, 0.7325980072793034, 0.45415120074586995, 0.8023447211948674, 0.31442819587787657, 1.0, 0.6397446104426214, 0.15969042033574304, 0.2909824112973824, 0.7078784636576176, 0.3523170414140183, 1.0, 0.5523734566484906, 0.6138901382091895, 0.3077239095403343, 0.5974938657418302, 0.35736419903847805, 0.9799185207517752, 0.3886871182566616, 0.39153142533542024, 1.0, 0.6675343210625576, 0.4387487708650044, 0.9448195884382898, 0.18977313710278615, 0.4013037603960307, 1.0, 0.6246894990227724, 4], 
[1.0, 0.6759645565393406, 0.4650887573964497, 1.0, 0.7044856998064788, 0.6643470401095601, 0.3317666801577745, 0.5397671428268402, 0.6047369821779582, 0.9496152968318257, 0.5900854243281837, 0.613703974433705, 0.5356484235686402, 0.0, 0.6165582268875486, 1.0, 0.5739659742816383, 0.5364259578392618, 1.0, 0.9168347816873671, 0.4517658217125639, 1.0, 0.5063343581154667, 0.3906264080618809, 1.0, 0.7660202316650389, 0.6830962934576584, 1.0, 0.4448836433516272, 0.21276670817190715, 1.0, 0.7346124718012746, 0.6633788266124641, 5], 
[0.7694219395593686, 0.6948825619163761, 0.0, 0.630075593417795, 0.5200654870392258, 1.0, 0.3424582947126371, 0.2151220810342085, 1.0, 0.9468323853012347, 0.3647471812421667, 1.0, 0.1076456952246061, 0.3779383499531065, 1.0, 0.806637046117703, 0.3113986616867348, 1.0, 0.0, 0.4097472478724519, 1.0, 0.6470115447865661, 0.21683932085519111, 1.0, 0.0, 0.23114458461836068, 0.8658050143608639, 0.6007294467625253, 0.1614155712357141, 0.491101413242003, 0.0, 0.32303227353453395, 0.7236732932297363, 10], 
[0.0, 0.05554874041293713, 0.6597633136094674, 0.028403067583964367, 0.0, 0.2754652284312073, 0.7569100540916246, 0.6010615732695126, 0.4052572646334914, 0.5123612457398913, 0.0, 0.42145268181080264, 0.6547974637764017, 0.614069200335208, 0.44992491544039903, 0.2575431989436257, 0.0, 0.5079535882930298, 0.8660982141003704, 0.669877166908382, 0.5100675188960899, 0.0018510956431624463, 0.0, 0.8382517708395989, 0.5689479969581129, 0.6021175211304348, 0.7436573665888233, 0.0, 0.0, 1.0, 0.44274709441957194, 0.5111813894920846, 0.9595715838267318, 13], 
[0.1465806340487799, 0.0, 1.0, 0.09316827927901845, 0.0894399241277042, 0.0, 0.6877916660055574, 1.0, 0.0, 0.9068546168069124, 0.3363566019910783, 0.0, 1.0, 1.0, 0.0, 0.6997800152680589, 0.26104283709984644, 0.0, 0.7923624918166629, 0.828569351832134, 0.23670937550271381, 0.4785574069571494, 0.19608233169857445, 0.1538467221593404, 0.5951826407566216, 0.5947280103542418, 0.6322007339703268, 0.42262466498682166, 0.13702096571689146, 0.6667745877298819, 0.47143975749182077, 0.601125619972926, 0.8476956868428115, 9], 
[0.7043509110219428, 0.49517296965177804, 0.04142011834319525, 0.5984132965781629, 0.21265817948104268, 0.7772734969633461, 0.8185308484364064, 0.13600616219977832, 0.896199216343106, 0.5923391828716542, 0.22657469878686584, 0.7774733058779788, 0.36780557006697256, 0.4918176448806539, 0.7541690842150458, 0.6340367172696312, 0.2946013932305495, 0.7658305764096837, 0.56891406312528, 0.589547224046454, 0.8655851977085932, 0.6154119639124657, 0.29153477893725505, 0.7411904026545748, 0.6149337190464065, 0.4906186799080091, 0.8975014322603394, 0.6779161384365078, 0.28672884802755, 0.424456813617819, 0.6496680799700665, 0.5694798337081582, 0.9101523358566128, 8], 
[0.7451223451282283, 0.9263575189172127, 0.17337278106508877, 0.5224869535743728, 1.0, 0.5806431513345518, 0.0, 0.5026324152681811, 0.4913024784861286, 0.10846099582039673, 0.7152113523588464, 0.7267859567605927, 0.20966198781127354, 0.3983322485185187, 0.7470988938065082, 0.2226179282571477, 0.6872082767225381, 0.7273181754732144, 0.34481290582128427, 0.8829001293538558, 0.9481676956512193, 0.35160405551358015, 0.5897936751282497, 0.7066957748332735, 0.49380058619729833, 0.7147203912673026, 0.9921321857613986, 0.5734549921006772, 0.5118777415954047, 0.4062324200717858, 0.6085624731525724, 0.7348565419135826, 0.8984359690895576, 2], 
[0.750722498020255, 0.8102423606414776, 0.7071005917159764, 0.8092526412946168, 0.8890774259705665, 0.4925404171734602, 1.0, 0.717845353620434, 0.32126517677774385, 0.6166752559734396, 0.8131387190039332, 0.4077568819796956, 0.13983812668463824, 0.0850412908896176, 0.36730705365312427, 0.6935649017814259, 0.7353577290681848, 0.2728569834444385, 0.5456437447737262, 0.0, 0.0, 0.7164902855840808, 0.7042919735233222, 0.0, 0.9457046775491866, 0.0, 0.0, 0.7593529394334577, 0.6972394326920854, 0.0, 0.857345120814506, 0.7791548743737293, 0.0, 7], 
[0.2391198973825141, 0.5596505059810688, 0.062426035502958575, 0.0, 0.2642738113248016, 0.7519473807653625, 0.12286099154132306, 0.0, 0.7572030833706854, 0.07499423142621175, 0.39071792482325796, 0.7110374742816844, 0.13877858480358563, 0.6433627715079143, 0.7423790190579348, 0.05475445317067581, 0.5335506461809615, 0.6842554231458385, 0.12508677712709995, 0.7878496993186728, 0.6841912606307579, 0.019280196908454162, 0.5629320872404673, 0.6534715801744364, 0.12370378069608523, 0.6665109439699813, 0.7697457580076228, 0.14393464525154673, 0.5565494267529569, 0.42175545566780165, 0.6980496912527802, 0.4326560047386468, 0.7104137591070935, 6]
]
filename = 'landmark_data_less.csv'
dataset = load_csv(filename)
for i in range(len(dataset[0])-1):
	str_column_to_float(dataset, i)
# convert class column to integers
str_column_to_int(dataset, len(dataset[0])-1)
minmax = dataset_minmax(dataset)
normalize_dataset(dataset, minmax)

n_inputs = len(dataset[0]) - 1
n_outputs = len(set([row[-1] for row in dataset]))
network = initialize_network(n_inputs, 5, n_outputs)
print("starting train")
train_network(network, dataset, 0.05, 10, n_outputs)

for layer in network:
	print(layer)

for row in testDataSet:
	prediction = predict(network, row)
	print('Expected=%d, Got=%d' % (row[-1], prediction))


# # Test Backprop on Seeds dataset
# seed(1)
# # load and prepare data
# filename = 'landmark_test_data_less.csv'
# dataset = load_csv(filename)
# for i in range(len(dataset[0])-1):
# 	str_column_to_float(dataset, i)
# # convert class column to integers
# str_column_to_int(dataset, len(dataset[0])-1)
# # normalize input variables
# minmax = dataset_minmax(dataset)
# normalize_dataset(dataset, minmax)
# print(dataset)
# print("starting evalutaion")
# # evaluate algorithm
# n_folds = 5
# l_rate = 0.3
# n_epoch = 500
# n_hidden = 5
# scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)
# print('Scores: %s' % scores)
# print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))